# -*- coding: utf-8 -*-
"""Time-Series-Climate-Forecasting-using-LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TwnJyjFKn86oQrWCJCWHoSa6ZeCcpaSQ

# **Climate Forecasting With LSTM**

**Jena Climate** dataset is made up of 14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. This dataset covers data from January 1st 2009 to December 31st 2016

## Import Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator


import tensorflow as tf
import os
plt.style.use('fivethirtyeight')

import warnings
warnings.filterwarnings('ignore')

"""## Load Dataset

### Data Overview

| Index | Feature         | Format            | Description                                                                                     |
|-------|-----------------|-------------------|-----------------------------------------------------------------------------------------------|
| 1     | Date Time       | `01.01.2009 00:10:00` | Date-time reference                                                                            |
| 2     | p (mbar)        | `996.52`          | Atmospheric pressure in millibars                                                              |
| 3     | T (degC)        | `-8.02`           | Temperature in Celsius                                                                         |
| 4     | Tpot (K)        | `265.4`           | Temperature in Kelvin                                                                          |
| 5     | Tdew (degC)     | `-8.9`            | Dew point temperature in Celsius (absolute water content in air)                              |
| 6     | rh (%)          | `93.3`            | Relative humidity (%), indicating how saturated the air is with water vapor                   |
| 7     | VPmax (mbar)    | `3.33`            | Saturation vapor pressure (maximum vapor pressure at a given temperature)                     |
| 8     | VPact (mbar)    | `3.11`            | Actual vapor pressure                                                                         |
| 9     | VPdef (mbar)    | `0.22`            | Vapor pressure deficit (difference between saturation and actual vapor pressure)             |
| 10    | sh (g/kg)       | `1.94`            | Specific humidity (mass of water vapor per unit mass of air)                                  |
| 11    | H2OC (mmol/mol) | `3.12`            | Water vapor concentration (moles of water vapor per mole of air)                              |
| 12    | rho (g/mÂ³)      | `1307.75`         | Air density                                                                                    |
| 13    | wv (m/s)        | `1.03`            | Wind speed                                                                                     |
| 14    | max. wv (m/s)   | `1.75`            | Maximum wind speed                                                                            |
| 15    | wd (deg)        | `152.3`           | Wind direction in degrees                                                                      |
"""

zip_path = tf.keras.utils.get_file(
    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',
    fname='jena_climate_2009_2016.csv.zip',
    extract=True)

#extracted path/ directory
extracted_dir = zip_path.replace('.zip', '')

csv_file_path = os.path.join(extracted_dir, 'jena_climate_2009_2016.csv')

# load the dataset
df = pd.read_csv(csv_file_path)

df.head(2)

df.info()

"""## **Data Cleaning**

> Replace the spaces in the column names with underscores
"""

df.columns = df.columns.map(lambda x : x.replace(' ', '_').replace('.',''))

cols = list(df.columns)
for col in cols:
    unique_count = df[col].nunique()
    most_common_value = df[col].mode().values[0]  # Get the first mode
    null_count = df[col].isnull().sum()
    # max_value = df_joined[col].max()
    # min_value = df_joined[col].min()

    print(f'Column {col} has {unique_count} unique values, most common value: {most_common_value}, & {null_count} null values' )
    print('')

"""> Craete added date features as it will give more information to our training."""

def date_features(df):
    df['date'] = pd.to_datetime(df['Date_Time'], format='%d.%m.%Y %H:%M:%S')
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['week'] = df['date'].dt.isocalendar().week
    df['quarter']= df['date'].dt.quarter
    df['day_of_year'] = df['date'].dt.dayofyear
    df['day_of_week'] = df['date'].dt.dayofweek
    df['hour'] = df['date'].dt.hour
    df['minute'] = df['date'].dt.minute
    df['second'] = df['date'].dt.second
    #sines values to show cycles
    df["month_sin"] = round(np.sin(2 * np.pi * df["month"] / 12),2)
    df["month_cos"] = round(np.cos(2 * np.pi * df["month"] / 12),2)
    df.drop('date', axis=1, inplace=True)
    return df

df = date_features(df)
df.head(2)

"""> Set the times stamp column ```Date Time``` as the index column."""

df.index = pd.to_datetime(df['Date_Time'], format='%d.%m.%Y %H:%M:%S')
df.drop('Date_Time', axis=1, inplace=True)

"""## **Data Analysis**

> Plot the characterstics of each column through time.
"""

#cols = list(df.columns not in [])
cols = ['p_(mbar)','T_(degC)','Tpot_(K)',
      'Tdew_(degC)','rh_(%)','VPmax_(mbar)','VPact_(mbar)','VPdef_(mbar)',
      'sh_(g/kg)','H2OC_(mmol/mol)','rho_(g/m**3)']

custom_colors = ['#1f77b4']

for col in cols:
  df[col] = pd.to_numeric(df[col], errors='coerce')
  df[col].fillna(df[col].mean(), inplace=True)
  df[col] = df[col].astype(float)

  plt.figure(figsize=(12, 4))
  df[col].plot(subplots=True,linestyle='dashed', color='#34495e', linewidth=2, markersize=12,markerfacecolor='#f1c40f')
  plt.title(f'Time Series Plot of {col}',fontsize=12)
  plt.xlabel('Time', fontsize=8)
  plt.ylabel(col, fontsize=8)
  plt.tick_params(axis='both', labelsize=6)
  plt.grid(False)
  plt.gca().set_facecolor('#ecf0f1')
  plt.tight_layout()
  plt.show()

"""> Correlation tells the statistical relationship between the variables in the dataframe, whether small or not, negative or positive."""

numeric_df = df[cols].select_dtypes(include=['number']) #only select numerical values in teh dataframe
df_corr = numeric_df.corr() #pd.get_dummies(df.htype)
plt.figure(figsize=(12,4))
#Let's creaate a mask so as to not get the upper triagle of our heatmap for visual purpose.
mask = np.triu(np.ones(df_corr.shape),k=1)
np.fill_diagonal(df_corr.values, 0) # we set teh diagonal values to 0
sns.heatmap(data=df_corr,
            annot=True,
            annot_kws={"fontsize":8},
            fmt='.1f',
            cmap='BuGn',
            mask=mask,
            cbar=True
            #cbar_kws={"fontsize":1.2}
            )
plt.title("Correlation Heatmap", fontsize=10)
plt.xticks(fontsize=8)
plt.yticks(fontsize=8)
plt.grid(False)
plt.savefig('correlation_heatmap.png')
plt.show()

"""**Observation:**

- Clearly identifieble that there is a strong correlation between ```Air Pressure & Density``` variables and weather temperature.

## **Feature Engineering**

> The next step is to create features which is a very powerfull way to provide even more context to our model.

> For the purpose of this project, the ```**Temperature**``` was used. It is possible to use other variables to create more added features. However, can be time and resource intensive.
"""

class FeatureEngineer(BaseEstimator, TransformerMixin):
  """
    A feature engineering class for time series forecasting,

    Params:
    target_col (str): The column name of the target variable.

    Methods:
    fit(X, y=None): Fit the transformer to the data.
    transform(X): Transform the input data. Addiing Lag, Rolling, & Safe Percentage Change features.

    Returns:
    X_new (pd.DataFrame): The transformed feature-engineered data.
  """


    def __init__(self, target_col='T_(degC)'):
        self.target_col = target_col

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_new = X.copy()
        target = X_new[self.target_col]

        # ----- Lag Features -----
        for days in [1, 7, 14]:  # [1, 7,14] day lags
            X_new[f'{self.target_col}_lag_{days}d'] = target.shift(days*144)

        # ----- Rolling Features -----
        for days in [1, 7, 14]:
            window = days*144
            X_new[f'{self.target_col}_rollmean_{days}d'] = target.rolling(window, min_periods=1).mean()
            X_new[f'{self.target_col}_rollstd_{days}d'] = target.rolling(window, min_periods=1).std()

        # ----- Safe Percentage Change -----
        X_new[f'{self.target_col}_pct_change'] = target.pct_change().replace([np.inf, -np.inf], np.nan) #infine values need to be replaced other wise throws error

        # ----- Wind Direction needs fixing other wise throws error -
        if 'wd_(deg)' in X.columns:
            # have to convert to radians first
            wd_rad = np.deg2rad(X_new['wd_(deg)'])
            X_new['wd_sin'] = np.sin(wd_rad)
            X_new['wd_cos'] = np.cos(wd_rad)
            X_new = X_new.drop('wd_(deg)', axis=1)

        #remove null values
        X_new = X_new.replace([np.inf, -np.inf], np.nan)
        X_new = X_new.fillna(method='ffill').fillna(method='bfill')

        return X_new

def train_val_test(df):
  """
  Splits input dataframe to training, validation, & test sets

  Params:
    df (pd.DataFrame)

  Returns:
    train_df (pd.DataFrame)
    validation_df (pd.DataFrame)
    test_df (pd.DataFrame)

  """
  df = df.drop(columns=['Tpot_(K)'])
  train_split = int(len(df)* 0.7)
  validation_split = int(len(df)* 0.1)

  train_df = df[:train_split]
  validation_df = df[train_split: validation_split + train_split]
  test_df = df[validation_split + train_split:]
  return train_df, validation_df, test_df

train_df, validation_df, test_df = train_val_test(df)

def xy_splitter(train, validation, test):
    """
    Splits the training, validation, and test DataFrames into feature and target sets.

    Params:
    train (pd.DataFrame): The training DataFrame.
    validation (pd.DataFrame): The validation DataFrame.
    test (pd.DataFrame): The test DataFrame.

    Returns:
    X (pd.DataFrame): The feature set for the provided dataset.
    y (pd.Series): The target set for the provided dataset.
    """

    target = 'T_(degC)'
    X_train = train.drop(target, axis=1)
    y_train = train[target]

    X_test = test.drop(target, axis=1)
    y_test = test[target]

    X_val = validation.drop(target, axis=1)
    y_val = validation[target]

    print(f'Train shape: X_train {X_train.shape} & y_train {y_train.shape}')
    print(f'Validation shape: X_val {X_val.shape} & y_val {y_val.shape}')
    print(f'Test shape: X_test {X_test.shape} & y_test {y_test.shape}')

    return X_train, y_train, X_val, y_val, X_test, y_test

# create a class for Feature engineer and scaler
feature_engineer = FeatureEngineer()
scaler = StandardScaler()

#process the variables
train_processed = feature_engineer.fit_transform(train_df)
val_processed = feature_engineer.transform(validation_df)
test_processed = feature_engineer.transform(test_df)

X_train, y_train, X_val, y_val, X_test, y_test = xy_splitter(train_processed, val_processed, test_processed)

"""## **Scaling**"""

X_train = scaler.fit_transform(train_processed)
X_val = scaler.transform(val_processed)
X_test = scaler.transform(test_processed)

# Calculate how many rows were dropped during processing
train_dropped = len(X_train) - len(train_processed)
val_dropped = len(X_val) - len(val_processed)
test_dropped = len(X_test) - len(test_processed)

# Align targets by removing same number of rows from START
y_train_aligned = y_train.iloc[train_dropped:]
y_val_aligned = y_val.iloc[val_dropped:]
y_test_aligned = y_test.iloc[test_dropped:]

"""## **Time-Series Steps**"""

look_back = 14
batch_size = 64

#training
train_gen = TimeseriesGenerator(
    data=X_train,
    targets=y_train_aligned,
    length=look_back,
    batch_size=batch_size,
    shuffle=False
      )

#Validation dataset timesieries gerator
val_gen = TimeseriesGenerator(
    data=X_val,
    targets=y_val_aligned,
    length=look_back,
    batch_size=batch_size,
    shuffle=False
)

#test timesieries
test_gen = TimeseriesGenerator(
    data=X_test,
    targets=y_test_aligned,
    length=look_back,
    batch_size=batch_size,
    shuffle=False
)

"""## **Modelling**"""

#shape from processed features
n_features = X_train.shape[1]

model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(look_back, n_features)))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.ReLU())
model.add(tf.keras.layers.LSTM(32, return_sequences=False))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(1))

model.compile(loss='mse', optimizer='adam')

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),
    tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)
    ]

model.summary()

history = model.fit(
    train_gen,
    epochs=25,
    validation_data=val_gen,
    callbacks=callbacks
)

#model loss per training epoch
plt.figure(figsize=(12, 4))
plt.plot(history.history['loss'], label='Train Loss',color='#1B4D3E', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation Loss',color='#E9762B', linewidth=2,)
plt.title('Model Loss',fontsize=12)
plt.xlabel('Epochs',fontsize=8)
plt.ylabel('MSE Loss',fontsize=8)
plt.xticks(fontsize=8)
plt.yticks(fontsize=8)
plt.legend(fontsize=8)
plt.grid(alpha=0.3)
plt.savefig('model_loss.png')
plt.show()

"""## **Prediction**"""

# Use the best performing model saved as best_mode.keras
best_model = load_model('best_model.keras')
test_preds = best_model.predict(test_gen)

y_pred = model.predict(test_gen)
y_actual = y_test_aligned[look_back:]

#plot prediction performance
plt.figure(figsize=(12, 4))
plt.plot(np.arange(len(y_actual)), y_actual, label='Actual', color='#E9762B', linewidth=2)
plt.plot(np.arange(len(y_pred)), y_pred, label='Predicted', color='#1B4D3E', linestyle='dashed', linewidth=1)

plt.title('Actual vs Predicted Values (Test Data)', fontsize=12)
plt.xlabel('Time Steps', fontsize=10)
plt.ylabel('Temperature Values(C)', fontsize=10)
plt.legend(fontsize=10)
plt.xticks(fontsize=8)
plt.yticks(fontsize=8)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('prediction_performance.png')
plt.show()

